{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "+ Feature engineering is to transform the data in such a way that the information content is easily exposed to the model.\n",
    "+ This statement can mean many things and highly depends on what exactly is \"the model\".0\n",
    "+ As we have seen, we are using many tools in combination to manipulate data. Thus far, we have encountered pandas, Dask, and sklearn in this course, but there are many more (PySpark, SQL, DAX, M, R, etc.)\n",
    "+ It is important to discuss which tools are the right ones, specifically in the context of data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform using pandas/Dask/SQL or sklearn?\n",
    "\n",
    "+ Depending on the perspective, the answer could be neither, pandas, or sklearn:\n",
    "\n",
    "    - Neither: \n",
    "        * Most join and filtering should be done closer to the source using a database or parquet/Dask operation. \n",
    "        * Map-Reduce and Group-by-Aggregate (\"data warehousing\") operations.\n",
    "        * Indexing and reshuffling.\n",
    "    - Pandas, Dask, or PySpark: \n",
    "        * Renames tasks.\n",
    "        * Use python libraries like pandas, Dask, or pySpark to add contemporaneous feature, time-series manipulation (for example, adding lags), parallel computation (using Dask or pySpark).\n",
    "        * Do not use these libraries for sample-dependent features.\n",
    "    - Use sklearn, pytorch:\n",
    "        * Use python libraries like sklearn or pytorch to add features that are sample-dependent like scaling and normalization, one-hot encoding, tokenization, and vectorization.\n",
    "        * Model-depdenent transformations: PCA, embeddings, iterative/knn imputation, etc.\n",
    "+ Decisions must be guided by optimization criteria (time and resources) while avoiding data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Transforms in sklearn\n",
    "\n",
    "The list below is found in [Scikit's Documentation](https://scikit-learn.org/stable/modules/preprocessing.html), which also includes convenience interfaces for the classes below.\n",
    "\n",
    "Work with categorical variables:\n",
    "\n",
    "+ `preprocessing.Binarizer(*[, threshold, copy])`: Binarize data (set feature values to 0 or 1) according to a threshold.\n",
    "+ `preprocessing.KBinsDiscretizer([n_bins, ...])`:  Bin continuous data into intervals.\n",
    "+ `preprocessing.LabelBinarizer(*[, neg_label, ...])`: Binarize labels in a one-vs-all fashion.\n",
    "+ `preprocessing.LabelEncoder()`: Encode target labels with value between 0 and n_classes-1.\n",
    "+ `preprocessing.MultiLabelBinarizer(*[, ...])`:  Transform between iterable of iterables and a multilabel format.\n",
    "+ `preprocessing.OneHotEncoder(*[, categories, ...])`: Encode categorical features as a one-hot numeric array.\n",
    "+ `preprocessing.OrdinalEncoder(*[, ...])`: Encode categorical features as an integer array.\n",
    "\n",
    "Scale and normalize:\n",
    "\n",
    "+ `preprocessing.StandardScaler(*[, copy, ...])`: Standardize features by removing the mean and scaling to unit variance.\n",
    "+ `preprocessing.MaxAbsScaler(*[, copy])`: Scale each feature by its maximum absolute value.\n",
    "+ `preprocessing.MinMaxScaler([feature_range, ...])`: Transform features by scaling each feature to a given range.\n",
    "+ `preprocessing.Normalizer([norm, copy])`:  Normalize samples individually to unit norm.\n",
    "+ `preprocessing.RobustScaler(*[, ...])`: Scale features using statistics that are robust to outliers.\n",
    "\n",
    "\n",
    "Nonlinear transforms:\n",
    "\n",
    "+ `preprocessing.FunctionTransformer([func, ...])`: Constructs a transformer from an arbitrary callable.\n",
    "+ `preprocessing.KernelCenterer()`: Center an arbitrary kernel matrix \n",
    "+ `preprocessing.PolynomialFeatures([degree, ...])`: Generate polynomial and interaction features.\n",
    "+ `preprocessing.PowerTransformer([method, ...])`: Apply a power transform featurewise to make data more Gaussian-like.\n",
    "+ `preprocessing.QuantileTransformer(*[, ...])`: Transform features using quantiles information.\n",
    "+ `preprocessing.SplineTransformer([n_knots, ...])`: Generate univariate B-spline bases for features.\n",
    "+ `preprocessing.TargetEncoder([categories, ...])`: Target Encoder for regression and classification targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we doing?\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/04_column_transform_1.png\" width=\"75%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Objectives\n",
    "\n",
    "Build a pipeline that: \n",
    "\n",
    "+ Add indicators: \n",
    "\n",
    "    - SME indicated that a Debt-to-Ratio > 100% is too high.\n",
    "    - Missing values indicator for `monthly_income` and `num_dependents`.\n",
    "\n",
    "+ Impute missing values, where required.\n",
    "+ Standardize variables.\n",
    "+ Evaluate if a transform (Yeo-Johnson or Box-Cox) of selected variables (debt_ratio, monthly_income, and revolving_unsecured_line_utilization) is beneficial.\n",
    "\n",
    "Feature selection:\n",
    "\n",
    "+ We are looking for informative features: their contribution to prediction is valuable.\n",
    "+ We prefer parsimonious models.\n",
    "+ We want to retain evidence of our work and afford reproducibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "+ For this example, we will use [Give Me Some Credit from Kaggle](https://www.kaggle.com/c/GiveMeSomeCredit/data), a widely refered example. \n",
    "+ To run the examples below, download the data set and extract cs-training.csv to `../05_src/data/credit/`.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Solution\n",
    "\n",
    "+ To get deeper insights into the task, first approach it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation of simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, we obtain a log-loss of about 0.362."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Pipeline\n",
    "\n",
    "+ The pipeline below is more complex:\n",
    "\n",
    "    - Treat selected numericals using [Yeo-Johnson transformation](https://feature-engine.trainindata.com/en/latest/user_guide/transformation/YeoJohnsonTransformer.html).\n",
    "    - Treat other numericals with scaling only.\n",
    "    - Do not treat booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a greater loss of 0.443, therefore the additional feature is not profitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "+ We are currently evaluating two feature engineering procedures using the same classifier. \n",
    "\n",
    "    - However, feature engineering is classifier-dependent: each classifier is a specialized tool to learn a certain type of hypothesis. \n",
    "    - Different classifiers will benefit from different type of engineered features (see, for example, [Khun and Silge's recommendations on TMWR.org](https://www.tmwr.org/pre-proc-table)).\n",
    "\n",
    "+ We are producing data from our experiments.\n",
    "\n",
    "    - The data that we produced is more or less structured: we are using standard performance metrics, for instance.\n",
    "    - Each preprocessing pipeline will be different and may accept different configuration parameters.\n",
    "    - Likewise, classifiers will tend to have different configuration parameters. \n",
    "    \n",
    "+ We modify code to produce experiments:\n",
    "\n",
    "    - Our experiment results will be a function of our algorithm's logic, its implementation (code), and our data.\n",
    "    - Code tracking is doen with Git.\n",
    "    - Data tracking is in development.\n",
    "\n",
    "**It is generally a good idea to use software for experiment tracking once you move out of the Proof of Concept stage.** Some solutions include:\n",
    "\n",
    "- [ML Flow](https://mlflow.org/).\n",
    "- [Weights & Balances](https://wandb.ai/site).\n",
    "- [Sacred](https://sacred.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow\n",
    "\n",
    "+ MLFlow is a software tool that automates taks related to experiment tracking:\n",
    "\n",
    "    - Keep track of experiment parameters.\n",
    "    - Save configuration+s for individual experiment runs in files or databases.\n",
    "    - Store models and other artifacts to an object store.\n",
    "\n",
    "+ A few features that may be useful:\n",
    "\n",
    "    - Keep track of code and artifacts associated with experiment.\n",
    "    - Store experiment run times and system characteristics.\n",
    "    - Work with different backend stores (\"[Observers](https://mlflow.org/docs/latest/tracking/backend-stores)\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Experiment\n",
    "\n",
    "Continuing with our example, the following setup will track an experiment to measure the performance of a model pipeline. The main file for this experiment is `./05_src/credit/exp__logistic_simple.py`. You can run this experiment from the `05_src/` folder using `python -m credit.exp__logistic_simple`.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the experiment, take a look at MLFlow by navigating to [http://localhost:5001](http://localhost:5001).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
